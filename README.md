# **Video Short Creator Backend**

## **Overview**

The backend of this project is designed to process video transcripts and intelligently identify key sections of a video, allowing you to generate short, context-aware clips based on a set of given topics. This is achieved using a BERT-based model that analyzes the transcript, scores the similarity between chunks of text and the given topics, and expands context dynamically to capture complete ideas.

## **Features**

- **BERT-based Analysis**: Uses `sentence-transformers` to embed transcript chunks and compute semantic similarity with the provided topics.
- **Intelligent Context Expansion**: Expands context around high-relevance sections based on semantic similarity, ensuring that entire ideas are captured in the video shorts.
- **Flexible Customization**: Adjust thresholds and context windows for more fine-tuned results.
- **Video Clip Generation**: Outputs meaningful timestamp ranges that can be used to extract shorts from videos.

## **Prerequisites**

- Python 3.7 or higher
- pip package manager

## **Installation**

1. **Clone the repository**:

    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2. **Create a virtual environment** (optional but recommended):

    ```bash
    python3 -m venv venv
    source venv/bin/activate  # For Linux/Mac
    venv\Scripts\activate  # For Windows
    ```

3. **Install the required dependencies**:

    ```bash
    pip install -r requirements.txt
    ```

    The key dependencies are:
    - `sentence-transformers`: For BERT-based embeddings and semantic similarity.
    - `torch`: For deep learning and embedding generation.
    - `scikit-learn`: For cosine similarity calculations.
    - `fastapi` (if applicable for your API server): To serve the API for processing videos and transcripts.
    - `moviepy` (if applicable for video processing): For video clip extraction.

## **Configuration**

- You can adjust the default parameters (such as the model type, thresholds, and context window) directly within the `BERTAnalyzer` class in the backend code.

### **Key Parameters**:

- **model_name**: The pre-trained model to use from `sentence-transformers`. Default: `'paraphrase-MiniLM-L6-v2'`.
- **chunk_size**: Number of transcript sentences to group together into a chunk. Default: `7`.
- **threshold**: Minimum similarity score required for a chunk to be considered relevant. Default: `0.7`.
- **context_window**: Number of neighboring chunks to consider when expanding context. Default: `1`.
- **context_similarity_threshold**: Minimum similarity score required to expand the context intelligently. Default: `0.5`.

## **Usage**

### **Transcript Analysis**

The backend processes transcripts and generates timestamped segments of video where the topics are discussed. You can use the `calculate_similarity` method of the `BERTAnalyzer` class to generate intelligent video short timestamps.

1. **Example Code**:

    ```python
    from bert_analyzer import BERTAnalyzer

    # Initialize the analyzer
    analyzer = BERTAnalyzer()

    # Input data
    transcript = "Path to your transcript file or text input"
    topics = ["machine learning", "artificial intelligence"]

    # Analyze the transcript and generate timestamped clips
    results = analyzer.calculate_similarity(transcript, topics, threshold=0.7, context_similarity_threshold=0.5)

    # Output the results (timestamps and texts)
    print(results)
    ```

2. **Outputs**:

    The output will be a list of segments with the following structure:

    ```json
    [
        {
            "text": "Combined text from the expanded context",
            "score": 0.82,
            "start_timestamp": 60,
            "end_timestamp": 120
        },
        ...
    ]
    ```

    These results can be used to create video shorts by extracting the relevant clips from the original video using the `start_timestamp` and `end_timestamp`.

## **Video Processing (Optional)**

If you're generating the video clips programmatically, you can use `moviepy` or similar libraries to cut the videos based on the timestamps generated by the `BERTAnalyzer`.

1. **Example using `moviepy`**:

    ```python
    from moviepy.editor import VideoFileClip

    def extract_clip(video_path, start_time, end_time, output_path):
        with VideoFileClip(video_path) as video:
            clip = video.subclip(start_time, end_time)
            clip.write_videofile(output_path, codec="libx264")

    # Example usage
    extract_clip("input_video.mp4", start_time=60, end_time=120, output_path="short_clip.mp4")
    ```

## **API Integration (Optional)**

If youâ€™re running a FastAPI server to expose this functionality as an API, you can integrate `BERTAnalyzer` into your FastAPI routes.

1. **FastAPI Route Example**:

    ```python
    from fastapi import FastAPI, UploadFile, Form
    from bert_analyzer import BERTAnalyzer

    app = FastAPI()
    analyzer = BERTAnalyzer()

    @app.post("/process")
    async def process_video(transcript: str = Form(...), topics: str = Form(...)):
        topics_list = topics.split(",")  # Assume topics are passed as a comma-separated string
        results = analyzer.calculate_similarity(transcript, topics_list)
        return {"clips": results}
    ```

2. **Run the API**:

    ```bash
    uvicorn main:app --reload
    ```

    You can then send a POST request to `/process` with the transcript and topics to get the intelligent video clip timestamps.

## **Customizing for Your Needs**

Feel free to adjust the following parameters in the `BERTAnalyzer` class to fit your specific needs:
- **Model Type**: You can change the `model_name` to use a different pre-trained model from `sentence-transformers`.
- **Chunk Size and Thresholds**: Adjust the `chunk_size`, `threshold`, and `context_similarity_threshold` to fine-tune how much of the transcript is analyzed and included in the final output.

## **Troubleshooting**

1. **Large Timestamp Ranges**: If you notice overly large ranges of timestamps, reduce the `context_similarity_threshold` or cap the maximum range duration in the `expand_chunk_context` function.
2. **Low Accuracy**: Try different pre-trained models or adjust the similarity thresholds to capture more relevant results. For example, using a model like `all-MiniLM-L6-v2` may give different results depending on the input data.

## **Contributing**

Feel free to contribute to this project by opening issues, suggesting improvements, or submitting pull requests.

## **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

### **Contact Information**

For any questions, please feel free to reach out at [your-email@example.com].

---

This `README.md` provides a comprehensive guide to using the backend for transcript analysis and video clip generation, along with key configurations and usage examples.